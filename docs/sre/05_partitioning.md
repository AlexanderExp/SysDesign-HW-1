# 05. Партиционирование данных

> **Сложность:** ⭐⭐ Средний уровень
> **Вопросы из списка:** 23, 24, 25, 26

---

## Вопрос 23: Цели партиционирования

### Что такое партиционирование?

**Партиционирование (Sharding)** — это разделение данных на части, которые хранятся на разных серверах.

```
┌─────────────────────────────────────────────────────────┐
│                   ПАРТИЦИОНИРОВАНИЕ                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Было: Все данные на одном сервере                      │
│                                                         │
│  ┌───────────────────────────────────────┐              │
│  │         Все пользователи              │              │
│  │         (100 млн записей)             │              │
│  └───────────────────────────────────────┘              │
│                                                         │
│  Стало: Данные разделены по серверам                    │
│                                                         │
│  ┌───────────┐  ┌───────────┐  ┌───────────┐            │
│  │  Shard 1  │  │  Shard 2  │  │  Shard 3  │            │
│  │  A-H      │  │  I-P      │  │  Q-Z      │            │
│  │  33 млн   │  │  33 млн   │  │  34 млн   │            │
│  └───────────┘  └───────────┘  └───────────┘            │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Зачем нужно партиционирование?

#### 1. Масштабирование записи (Write Scaling)

```
┌─────────────────────────────────────────────────────────┐
│              МАСШТАБИРОВАНИЕ ЗАПИСИ                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Проблема: Один сервер не справляется с записью         │
│                                                         │
│  ┌─────────┐                                            │
│  │ Server  │ ← 100K записей/сек = ПЕРЕГРУЗКА!           │
│  └─────────┘                                            │
│                                                         │
│  Репликация НЕ помогает — все записи идут на Master     │
│                                                         │
│  Решение: Партиционирование                             │
│                                                         │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐     │
│  │ Shard 1 │  │ Shard 2 │  │ Shard 3 │  │ Shard 4 │     │
│  │  25K/s  │  │  25K/s  │  │  25K/s  │  │  25K/s  │     │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘     │
│                                                         │
│  Каждый shard обрабатывает часть записей!               │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

#### 2. Масштабирование хранения (Storage Scaling)

```
┌─────────────────────────────────────────────────────────┐
│              МАСШТАБИРОВАНИЕ ХРАНЕНИЯ                   │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Проблема: Данные не помещаются на один диск            │
│                                                         │
│  ┌─────────┐                                            │
│  │ Server  │ ← 10 TB диск, нужно 50 TB                  │
│  └─────────┘                                            │
│                                                         │
│  Решение: Разделить по серверам                         │
│                                                         │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐     │
│  │ Shard 1 │  │ Shard 2 │  │ Shard 3 │  │ Shard 4 │     │
│  │  10 TB  │  │  10 TB  │  │  10 TB  │  │  10 TB  │     │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘     │
│                     = 40 TB суммарно                    │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

#### 3. Параллелизм запросов

```
┌─────────────────────────────────────────────────────────┐
│                 ПАРАЛЛЕЛИЗМ ЗАПРОСОВ                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Аналитический запрос: SELECT COUNT(*) FROM orders      │
│                                                         │
│  Без партиционирования:                                 │
│  ┌─────────┐                                            │
│  │ Server  │ ← Сканирует 100 млн строк = 10 минут       │
│  └─────────┘                                            │
│                                                         │
│  С партиционированием:                                  │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐     │
│  │ Shard 1 │  │ Shard 2 │  │ Shard 3 │  │ Shard 4 │     │
│  │  25 млн │  │  25 млн │  │  25 млн │  │  25 млн │     │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘     │
│       │           │           │           │             │
│       └───────────┴───────────┴───────────┘             │
│                       │                                 │
│                  Параллельно!                           │
│                  = 2.5 минуты                           │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Партиционирование vs Репликация

```
┌─────────────────────────────────────────────────────────┐
│         ПАРТИЦИОНИРОВАНИЕ vs РЕПЛИКАЦИЯ                 │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  РЕПЛИКАЦИЯ:                                            │
│  • Копирует ВСЕ данные на несколько серверов            │
│  • Масштабирует ЧТЕНИЕ                                  │
│  • Не масштабирует запись и хранение                    │
│                                                         │
│  ПАРТИЦИОНИРОВАНИЕ:                                     │
│  • Делит данные на ЧАСТИ                                │
│  • Масштабирует ЗАПИСЬ и ХРАНЕНИЕ                       │
│  • Усложняет запросы к нескольким partitions            │
│                                                         │
│  НА ПРАКТИКЕ: Используют ОБА!                           │
│                                                         │
│  ┌─────────┐     ┌─────────┐     ┌─────────┐            │
│  │ Shard 1 │     │ Shard 2 │     │ Shard 3 │            │
│  │ Master  │     │ Master  │     │ Master  │            │
│  └────┬────┘     └────┬────┘     └────┬────┘            │
│       │               │               │                 │
│  ┌────┴────┐     ┌────┴────┐     ┌────┴────┐            │
│  │ Replica │     │ Replica │     │ Replica │            │
│  └─────────┘     └─────────┘     └─────────┘            │
│                                                         │
│  Каждый shard реплицируется для отказоустойчивости!     │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Сводка целей

| Цель | Что решает | Пример |
|------|------------|--------|
| **Write Scaling** | Масштабирование записи | 4 shards = 4x записей/сек |
| **Storage Scaling** | Больше места | 4 shards = 4x дискового пространства |
| **Query Parallelism** | Быстрые запросы | Параллельное сканирование |
| **Isolation** | Изоляция нагрузки | Разные клиенты на разных shards |

---

## Вопрос 24: Проблемы распределения данных при партиционировании

### Как решить, куда положить данные?

Нужен **Partition Key** (ключ партиционирования) — поле, по которому определяется shard.

### Стратегия 1: Range Partitioning (по диапазону)

```
┌─────────────────────────────────────────────────────────┐
│                  RANGE PARTITIONING                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Partition Key: user_id                                 │
│                                                         │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐│
│  │   Shard 1     │  │   Shard 2     │  │   Shard 3     ││
│  │ user_id 1-1M  │  │ user_id 1M-2M │  │ user_id 2M-3M ││
│  └───────────────┘  └───────────────┘  └───────────────┘│
│                                                         │
│  ✅ Плюсы:                                              │
│  • Эффективные range-запросы                            │
│  • Легко добавить новый shard в конец                   │
│                                                         │
│  ❌ Минусы:                                             │
│  • Hotspots! (новые user_id идут на последний shard)    │
│  • Неравномерное распределение                          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

**Проблема Hotspot:**

```
Время: 10:00                      Время: 11:00
                                  
┌───────┐ ┌───────┐ ┌───────┐     ┌───────┐ ┌───────┐ ┌───────┐
│Shard 1│ │Shard 2│ │Shard 3│     │Shard 1│ │Shard 2│ │Shard 3│
│ 10%   │ │ 10%   │ │ 80%   │     │ 10%   │ │ 10%   │ │ 80%   │
│       │ │       │ │ HOT!  │     │       │ │       │ │ HOT!  │
└───────┘ └───────┘ └───────┘     └───────┘ └───────┘ └───────┘

Все новые пользователи идут на Shard 3!
```

### Стратегия 2: Hash Partitioning (по хешу)

```
┌─────────────────────────────────────────────────────────┐
│                   HASH PARTITIONING                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Partition Key: user_id                                 │
│  Shard = hash(user_id) % num_shards                     │
│                                                         │
│  user_id=123 → hash(123) = 456 → 456 % 3 = 0 → Shard 0  │
│  user_id=124 → hash(124) = 789 → 789 % 3 = 0 → Shard 0  │
│  user_id=125 → hash(125) = 234 → 234 % 3 = 2 → Shard 2  │
│                                                         │
│  ✅ Плюсы:                                              │
│  • Равномерное распределение                            │
│  • Нет hotspots                                         │
│                                                         │
│  ❌ Минусы:                                             │
│  • Range-запросы неэффективны (scatter-gather)          │
│  • При добавлении shard — перераспределение данных      │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Стратегия 3: Consistent Hashing

```
┌─────────────────────────────────────────────────────────┐
│                  CONSISTENT HASHING                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Идея: Хеш-кольцо, где и ключи, и серверы имеют позиции │
│                                                         │
│                    0°                                   │
│                    │                                    │
│              ┌─────┼─────┐                              │
│             /      │      \                             │
│           Shard A  │       Shard B                      │
│          /         │         \                          │
│        90°─────────┼─────────270°                       │
│          \         │         /                          │
│           Shard D  │       Shard C                      │
│             \      │      /                             │
│              └─────┼─────┘                              │
│                   180°                                  │
│                                                         │
│  Ключ → hash → позиция на кольце → ближайший shard      │
│                                                         │
│  ✅ При добавлении/удалении shard перемещается          │
│     только часть данных (1/N), а не все!                │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

**Добавление нового shard:**

```
До:                              После:
     ┌───────────┐                    ┌───────────┐
    A│   50%     │B                  A│   33%     │B
     │           │                    │     E     │
     │           │                    │   17%     │
     └───────────┘                    └───────────┘
    D     50%     C                  D    33%      C

Только данные между D и E переместились!
```

### Проблема: Skew (перекос данных)

```
┌─────────────────────────────────────────────────────────┐
│                    DATA SKEW                            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Проблема: Некоторые ключи "популярнее" других          │
│                                                         │
│  Пример: Twitter                                        │
│  • Partition by user_id                                 │
│  • У @elonmusk 100M фолловеров                          │
│  • Его shard перегружен!                                │
│                                                         │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐                  │
│  │ Shard 1 │  │ Shard 2 │  │ Shard 3 │                  │
│  │  1K qps │  │  1K qps │  │ 100K qps│ ← @elonmusk      │
│  └─────────┘  └─────────┘  └─────────┘                  │
│                                                         │
│  Решения:                                               │
│  • Добавить случайный суффикс к hot key                 │
│  • Кэширование hot data                                 │
│  • Специальная обработка celebrity users                │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Выбор Partition Key

```
┌─────────────────────────────────────────────────────────┐
│                ВЫБОР PARTITION KEY                      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Хороший ключ:                                          │
│  ✅ Высокая кардинальность (много уникальных значений)  │
│  ✅ Равномерное распределение                           │
│  ✅ Часто используется в запросах                       │
│  ✅ Неизменяемый (иначе — перемещение данных)           │
│                                                         │
│  Плохой ключ:                                           │
│  ❌ Низкая кардинальность (country — мало значений)     │
│  ❌ Неравномерный (created_date — все новые в одном)    │
│  ❌ Редко в запросах (придётся scatter-gather)          │
│                                                         │
│  Примеры хороших ключей:                                │
│  • user_id для данных пользователя                      │
│  • order_id для заказов                                 │
│  • tenant_id для multi-tenant систем                    │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## Вопрос 25: Маршрутизация запросов (Query Routing)

### Проблема: Куда отправить запрос?

```
┌─────────────────────────────────────────────────────────┐
│                   QUERY ROUTING                         │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Клиент хочет: SELECT * FROM users WHERE user_id = 123  │
│                                                         │
│  Вопрос: На каком shard лежит user_id=123?              │
│                                                         │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐                  │
│  │ Shard 1 │  │ Shard 2 │  │ Shard 3 │                  │
│  │   ???   │  │   ???   │  │   ???   │                  │
│  └─────────┘  └─────────┘  └─────────┘                  │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Подход 1: Клиент знает маршрутизацию

```
┌─────────────────────────────────────────────────────────┐
│              CLIENT-SIDE ROUTING                        │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Клиент сам вычисляет: shard = hash(user_id) % 3        │
│                                                         │
│  ┌────────┐                                             │
│  │ Client │──── user_id=123 → hash → Shard 2            │
│  └────────┘                                             │
│       │                                                 │
│       └────────────────────────► ┌─────────┐            │
│                                  │ Shard 2 │            │
│                                  └─────────┘            │
│                                                         │
│  ✅ Плюсы: Быстро, нет лишних хопов                     │
│  ❌ Минусы: Клиент должен знать топологию               │
│                                                         │
│  Примеры: Redis Cluster, Cassandra drivers              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Подход 2: Routing Tier (прокси)

```
┌─────────────────────────────────────────────────────────┐
│                   ROUTING TIER                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Прокси знает, где какие данные                         │
│                                                         │
│  ┌────────┐      ┌─────────┐                            │
│  │ Client │─────►│  Proxy  │                            │
│  └────────┘      │ (Router)│                            │
│                  └────┬────┘                            │
│                       │                                 │
│         ┌─────────────┼─────────────┐                   │
│         ▼             ▼             ▼                   │
│    ┌─────────┐  ┌─────────┐  ┌─────────┐               │
│    │ Shard 1 │  │ Shard 2 │  │ Shard 3 │               │
│    └─────────┘  └─────────┘  └─────────┘               │
│                                                         │
│  ✅ Плюсы: Клиент простой, логика централизована        │
│  ❌ Минусы: Дополнительный хоп, точка отказа            │
│                                                         │
│  Примеры: Vitess, ProxySQL, mongos                      │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Подход 3: Любой узел маршрутизирует

```
┌─────────────────────────────────────────────────────────┐
│              ANY-NODE ROUTING                           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Клиент подключается к любому узлу, тот перенаправляет  │
│                                                         │
│  ┌────────┐      ┌─────────┐                            │
│  │ Client │─────►│ Shard 1 │ ─── "Не мои данные,        │
│  └────────┘      └─────────┘      иди на Shard 2"       │
│       │                                │                │
│       └────────────────────────────────▼                │
│                                  ┌─────────┐            │
│                                  │ Shard 2 │            │
│                                  └─────────┘            │
│                                                         │
│  ✅ Плюсы: Простота, нет отдельного прокси              │
│  ❌ Минусы: Возможны лишние хопы                        │
│                                                         │
│  Примеры: Cassandra, CockroachDB                        │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Scatter-Gather запросы

```
┌─────────────────────────────────────────────────────────┐
│                   SCATTER-GATHER                        │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Запрос без partition key: SELECT * FROM users          │
│                            WHERE age > 25               │
│                                                         │
│  Нужно спросить ВСЕ shards!                             │
│                                                         │
│  ┌────────┐      ┌─────────┐                            │
│  │ Client │─────►│ Router  │                            │
│  └────────┘      └────┬────┘                            │
│                       │                                 │
│         ┌─────────────┼─────────────┐  SCATTER          │
│         ▼             ▼             ▼                   │
│    ┌─────────┐  ┌─────────┐  ┌─────────┐               │
│    │ Shard 1 │  │ Shard 2 │  │ Shard 3 │               │
│    └────┬────┘  └────┬────┘  └────┬────┘               │
│         │            │            │                     │
│         └────────────┼────────────┘  GATHER             │
│                      ▼                                  │
│               ┌───────────┐                             │
│               │  Merge    │                             │
│               │  Results  │                             │
│               └───────────┘                             │
│                                                         │
│  ⚠️ Медленно! Избегай запросов без partition key        │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## Вопрос 26: Ребалансировка (Rebalancing)

### Зачем нужна ребалансировка?

```
┌─────────────────────────────────────────────────────────┐
│                    REBALANCING                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Когда нужно:                                           │
│  • Добавили новый shard (масштабирование)               │
│  • Удалили shard (уменьшение)                           │
│  • Shard перегружен (hotspot)                           │
│  • Shard упал (failover)                                │
│                                                         │
│  Задача: Перераспределить данные равномерно             │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Стратегия 1: Фиксированное число партиций

```
┌─────────────────────────────────────────────────────────┐
│              FIXED NUMBER OF PARTITIONS                 │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Идея: Создать МНОГО партиций заранее (например, 1000)  │
│        и распределить по узлам                          │
│                                                         │
│  3 узла, 12 партиций:                                   │
│  Node 1: [P1, P2, P3, P4]                               │
│  Node 2: [P5, P6, P7, P8]                               │
│  Node 3: [P9, P10, P11, P12]                            │
│                                                         │
│  Добавили Node 4:                                       │
│  Node 1: [P1, P2, P3]      ← отдал P4                   │
│  Node 2: [P5, P6, P7]      ← отдал P8                   │
│  Node 3: [P9, P10, P11]    ← отдал P12                  │
│  Node 4: [P4, P8, P12]     ← получил по одной           │
│                                                         │
│  ✅ Плюсы: Простая ребалансировка                       │
│  ❌ Минусы: Нужно угадать число партиций заранее        │
│                                                         │
│  Примеры: Elasticsearch, Riak, Couchbase                │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Стратегия 2: Динамическое партиционирование

```
┌─────────────────────────────────────────────────────────┐
│              DYNAMIC PARTITIONING                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Идея: Партиции создаются и делятся автоматически       │
│                                                         │
│  Начало: 1 партиция                                     │
│  ┌─────────────────────────────────────┐                │
│  │              P1 (все данные)        │                │
│  └─────────────────────────────────────┘                │
│                                                         │
│  P1 выросла → Split:                                    │
│  ┌─────────────────┐  ┌─────────────────┐               │
│  │       P1        │  │       P2        │               │
│  │   (A-M)         │  │   (N-Z)         │               │
│  └─────────────────┘  └─────────────────┘               │
│                                                         │
│  P1 стала маленькой → Merge с соседом:                  │
│  ┌─────────────────────────────────────┐                │
│  │              P1 (A-Z)               │                │
│  └─────────────────────────────────────┘                │
│                                                         │
│  ✅ Плюсы: Адаптируется к данным                        │
│  ❌ Минусы: Сложнее реализовать                         │
│                                                         │
│  Примеры: HBase, MongoDB                                │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Стратегия 3: Партиция на узел

```
┌─────────────────────────────────────────────────────────┐
│              PARTITION PER NODE                         │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Идея: Фиксированное число партиций НА УЗЕЛ             │
│                                                         │
│  3 узла × 4 партиции = 12 партиций                      │
│  4 узла × 4 партиции = 16 партиций                      │
│                                                         │
│  При добавлении узла:                                   │
│  • Создаются новые партиции                             │
│  • Часть данных перемещается                            │
│                                                         │
│  Примеры: Cassandra (default)                           │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Процесс ребалансировки

```
┌─────────────────────────────────────────────────────────┐
│               ПРОЦЕСС РЕБАЛАНСИРОВКИ                    │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. Планирование                                        │
│     • Определить, какие партиции куда переместить       │
│     • Минимизировать перемещение данных                 │
│                                                         │
│  2. Копирование данных                                  │
│     • Фоновый процесс копирует данные                   │
│     • Система продолжает работать!                      │
│                                                         │
│  3. Переключение                                        │
│     • Атомарное обновление маршрутизации                │
│     • Новые запросы идут на новый узел                  │
│                                                         │
│  4. Очистка                                             │
│     • Удаление старых копий                             │
│                                                         │
│  ⚠️ Во время ребалансировки:                            │
│  • Повышенная нагрузка на сеть и диск                   │
│  • Возможны временные задержки                          │
│  • Не делай ребалансировку в пиковые часы!              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Автоматическая vs Ручная ребалансировка

```
┌─────────────────────────────────────────────────────────┐
│          АВТОМАТИЧЕСКАЯ vs РУЧНАЯ                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  АВТОМАТИЧЕСКАЯ:                                        │
│  ✅ Удобно, не нужно следить                            │
│  ❌ Может начаться в неподходящий момент                │
│  ❌ Может ошибочно среагировать на временную проблему   │
│                                                         │
│  РУЧНАЯ:                                                │
│  ✅ Полный контроль                                     │
│  ✅ Можно выбрать время                                 │
│  ❌ Требует внимания оператора                          │
│                                                         │
│  Рекомендация:                                          │
│  • Автоматическое обнаружение проблем                   │
│  • Ручное подтверждение ребалансировки                  │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## Ключевые термины

| Термин | Определение |
|--------|-------------|
| **Партиционирование** | Разделение данных на части по разным серверам |
| **Shard** | Одна часть (партиция) данных |
| **Partition Key** | Ключ, по которому определяется shard |
| **Range Partitioning** | Разделение по диапазону значений |
| **Hash Partitioning** | Разделение по хешу ключа |
| **Consistent Hashing** | Хеш-кольцо для минимизации перемещений |
| **Hotspot** | Перегруженный shard |
| **Scatter-Gather** | Запрос ко всем shards |
| **Rebalancing** | Перераспределение данных между shards |

---

## Что запомнить

1. **Партиционирование масштабирует запись и хранение** (в отличие от репликации)
2. **Hash partitioning = равномерность, Range = эффективные диапазоны**
3. **Consistent hashing минимизирует перемещение при добавлении узлов**
4. **Выбор partition key критичен** — плохой ключ = hotspots
5. **Scatter-gather медленный** — всегда включай partition key в запрос
6. **Ребалансировка — дорогая операция**, планируй заранее

---

*Следующий файл: [06. Микросервисы и распределённые транзакции](06_microservices_transactions.md)*